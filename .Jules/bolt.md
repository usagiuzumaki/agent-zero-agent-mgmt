## 2024-05-22 - [Tiktoken Overhead on Deltas]
**Learning:** `tiktoken` encoding has significant overhead (~0.1ms) relative to the token count for very short strings (1-5 chars). When `approximate_tokens` is called on every streaming delta (e.g. in `models.py`), it creates a measurable CPU bottleneck and, more importantly, summing these approximations leads to massive overestimation (2.25x) due to token fragmentation.
**Action:** Use a fast O(1) heuristic (e.g. `len(text) // 3`) for short strings (<100 chars) in utility functions. This is 80x faster and no less accurate for the purpose of rate limiting deltas.
